\chapter{Lezione 4}
\label{chap:lezione_04} 

\begin{flushright}
\textit{Data: 09/10/2025}
\end{flushright}


\section{Il Metodo della Matrice di Trasferimento}

In questa lezione continuiamo l'analisi del modello di Ising, introducendo una tecnica risolutiva per sistemi unidimensionali nota come il metodo della \textbf{Matrice di Trasferimento}. Questo approccio si rivela estremamente potente, non solo per il calcolo della funzione di partizione, ma anche per ottenere grandezze fisiche come le marginali e le funzioni di correlazione. L'idea fondamentale è quella di "propagare" l'informazione lungo il sistema, costruendo la soluzione un passo (o uno spin) alla volta.

Consideriamo un sistema di spin unidimensionale con condizioni al contorno periodiche (CBC). L'Hamiltoniana del sistema è data da:

\begin{equation}
H(\underline{S}) = -J \sum_{i} S_i S_{i+1} - h \sum_{i} S_i
\end{equation}

dove la somma è estesa a tutti gli spin del sistema. La funzione di partizione totale del sistema è:

\begin{equation}
Z = \sum_{\underline{S}} e^{-\beta H(\underline{S})}
\end{equation}

\subsubsection{La Funzione di Partizione Parziale}

Il cuore del metodo consiste nell'introdurre una \textbf{funzione di partizione parziale}, $Z_{i,j}(S_i, S_j)$, definita su un sotto-insieme di spin tra il sito $i$ e il sito $j$. Questa funzione non è un singolo numero, ma un oggetto che dipende dai valori degli spin ai bordi di questo sotto-insieme, $S_i$ e $S_j$. Fissati questi due valori, sommiamo su tutte le configurazioni degli spin interni.

\begin{equation}
Z_{i,j}(S_i, S_j) = \sum_{\{s_k\}_{i<k<j}} e^{-\beta H_{i,j}(\{s_k\})}
\end{equation}

Questa funzione contiene molta più informazione rispetto alla funzione di partizione totale definita sulla stessa regione. Infatti, mentre la funzione di partizione complessiva fornisce un unico valore che tiene conto di tutte le configurazioni possibili, la funzione parziale mantiene esplicitamente la dipendenza dagli spin ai bordi. 

Poiché ciascuno di essi può assumere due valori, $S_i, S_j \in \{+1, -1\}$, la funzione $Z_{i,j}(S_i, S_j)$ non rappresenta un solo numero ma un insieme di \emph{quattro} valori distinti:
\[
Z_{i,j}(S_i, S_j) \;\longrightarrow\; 
\begin{cases}
Z_{i,j}(+1, +1), \\
Z_{i,j}(+1, -1), \\
Z_{i,j}(-1, +1), \\
Z_{i,j}(-1, -1).
\end{cases}
\]
Ciascuno di questi termini corrisponde al contributo statistico della regione compresa tra $i$ e $j$ sotto specifiche condizioni ai bordi. In questo senso, $Z_{i,j}$ può essere interpretata come una \emph{matrice $2\times2$} che codifica l'influenza reciproca tra i due spin estremi.

La funzione di partizione totale $Z$ può essere recuperata da quella parziale. Ad esempio, considerando l'intera catena da $i=1$ a $j=N$ e imponendo la condizione periodica $S_{N+1} = S_1$, si ha:

\begin{equation}
Z = \sum_{S_1} Z_{1,N}(S_1, S_1)
\end{equation}


L'utilità della funzione di partizione parziale risiede nella possibilità di scrivere una relazione ricorsiva. Supponiamo di conoscere $Z_{i,j}(S_i, S_j)$ e di voler calcolare la funzione di partizione per una catena più lunga di uno spin, $Z_{i,j+1}(S_i, S_{j+1})$. Possiamo ottenerla aggiungendo l'interazione dello spin $S_j$ con il nuovo spin $S_{j+1}$ e sommando su tutti i possibili valori di $S_j$.

\begin{equation}
Z_{i,j+1}(S_i, S_{j+1}) = \sum_{S_j} Z_{i,j}(S_i, S_j) e^{\beta J S_j S_{j+1} + \beta h S_{j+1}}
\end{equation}

Questo passaggio definisce implicitamente una "matrice" che trasferisce l'informazione dal passo $j$ al passo $j+1$. Tuttavia, l'espressione $e^{\beta J S_j S_{j+1} + \beta h S_{j+1}}$ non è simmetrica rispetto allo scambio degli indici $j$ e $j+1$, il che potrebbe portare ad autovalori complessi, complicando l'analisi.

\subsubsection{Simmetrizzazione della Matrice di Trasferimento}

Per ovviare al problema della simmetria, possiamo riscrivere in modo equivalente l'Hamiltoniana del sistema, distribuendo il campo magnetico locale $h$ a metà su due interazioni adiacenti.

\begin{equation}
H(\underline{S}) = -J \sum_{i} S_i S_{i+1} - \frac{h}{2} \sum_{i} (S_i + S_{i+1})
\end{equation}

Con questa forma, il termine che si aggiunge a ogni passo diventa simmetrico. Definiamo quindi la \textbf{Matrice di Trasferimento} $T(S_j, S_{j+1})$ come:

\begin{equation}
T(S_j, S_{j+1}) = e^{\beta J S_j S_{j+1} + \frac{\beta h}{2} (S_j + S_{j+1})}
\end{equation}

Essendo gli spin binari ($S = \pm 1$), questa può essere rappresentata come una matrice $2 \times 2$, con righe indicizzate da $S_j$ e colonne da $S_{j+1}$.

\begin{equation}
T = 
\begin{pmatrix}
e^{\beta J + \beta h} & e^{-\beta J} \\
e^{-\beta J} & e^{\beta J - \beta h}
\end{pmatrix}
\end{equation}

Questa matrice è ora manifestamente simmetrica.

\subsubsection{Funzione di Partizione e Autovalori}

Utilizzando la matrice di trasferimento, la funzione di partizione totale per una catena di $N$ spin con condizioni periodiche può essere espressa elegantemente come la traccia della potenza $N$-esima di $T$.

\begin{equation}
Z = \sum_{S_1, \dots, S_N} T(S_1, S_2) T(S_2, S_3) \cdots T(S_N, S_1) = \text{Tr}(T^N)
\end{equation}

L'operazione di traccia implementa la chiusura del ciclo, ovvero l'identificazione di $S_{N+1}$ con $S_1$.

Poiché la matrice $T$ è simmetrica, ammette due autovalori reali, $\lambda_1$ e $\lambda_2$. La funzione di partizione diventa:

\begin{equation}
Z = \lambda_1^N + \lambda_2^N
\end{equation}

Nel limite termodinamico ($N \to \infty$), il termine dominante è quello associato all'autovalore più grande, che per convenzione chiamiamo $\lambda_1$ ($\lambda_1 > \lambda_2$).

\begin{equation}
Z \approx \lambda_1^N
\end{equation}

L'energia libera di Helmholtz per spin, $f = - (\frac{1}{\beta N}) \log Z$, è quindi determinata unicamente dall'autovalore principale.

\begin{equation}
f = -\frac{1}{\beta} \log \lambda_1
\end{equation}

Questo risultato è di fondamentale importanza: la complessità di una somma su $2^N$ stati è stata ridotta al calcolo dell'autovalore più grande di una piccola matrice $2 \times 2$. Svolgendo i calcoli per la matrice $T$, si ottiene:

\begin{equation}
    \lambda_1 = e^{\beta J} \left[\cosh(\beta h) + \sqrt{\sinh^2(\beta h) + e^{-4\beta J}}\right]
\end{equation}

\begin{equation}
    \lambda_2 = e^{\beta J} \left[\cosh(\beta h) - \sqrt{\sinh^2(\beta h) + e^{-4\beta J}}\right]
\end{equation}

L'espressione esatta per l'energia libera in presenza di campo magnetico è:

\begin{equation}
f = -J - \frac{1}{\beta} \log \left( \cosh(\beta h) + \sqrt{\sinh^2(\beta h) + e^{-4\beta J}} \right)
\end{equation}

\subsubsection{Suscettività Magnetica e Funzioni di Correlazione}

Avendo a disposizione l'energia libera per qualsiasi valore del campo $h$, possiamo calcolare la magnetizzazione $m$ e la suscettività magnetica $\chi_m$.

\begin{equation}
m = -\frac{\partial f}{\partial h} = \frac{\sinh(\beta h)}{\sqrt{\sinh^2(\beta h) + e^{-4\beta J}}}
\end{equation}

Per campi piccoli ($h \to 0$), la magnetizzazione risponde linearmente, $m \approx \chi_m h$. La suscettività risulta:

\begin{equation}
\chi_m \approx \beta e^{2\beta J}
\end{equation}

\subsection{Teorema di Fluttuazione-Dissipazione}

Esiste una relazione profonda e generale, nota come \textbf{Teorema di Fluttuazione-Dissipazione}, che collega la risposta di un sistema a una perturbazione esterna (la suscettività) alle fluttuazioni interne del sistema in equilibrio (la funzione di correlazione).

La suscettività è definita come $\chi = \frac{\partial m}{\partial h}$. Partendo dalla definizione di magnetizzazione media $\langle S_i \rangle$:

\begin{equation}
m = \frac{1}{N}\sum_i \langle S_i \rangle = \frac{1}{N} \sum_i \frac{\sum_{\underline{S}} S_i e^{-\beta H}}{\sum_{\underline{S}} e^{-\beta H}}
\end{equation}

Derivando rispetto ad $h$, e ricordando che $H = H_0 - h \sum_j S_j$, otteniamo:

\begin{equation}
\chi = \frac{\partial}{\partial h} \left(\frac{1}{N}\sum_i \langle S_i \rangle\right) = \frac{\beta}{N}\sum_{i,j} \left( \langle S_i S_j \rangle - \langle S_i \rangle \langle S_j \rangle \right)
\end{equation}

Sfruttando l'invarianza traslazionale del sistema, possiamo riscrivere la doppia somma come una singola somma sulla distanza relativa $r = j-i$:

\begin{equation}
\chi = \beta \sum_{r} \left( \langle S_0 S_r \rangle - \langle S_0 \rangle \langle S_r \rangle \right) = \beta \sum_r C(r)
\end{equation}

La suscettività è proporzionale all'integrale (o somma) della funzione di correlazione connessa $C(r)$. 

Dato che la funzione di correlazione per il modello di Ising 1D decade esponenzialmente con una forma del tipo $C(r) \propto e^{-|r|/\xi}$, possiamo approssimare la somma con un integrale nel continuo:

\begin{equation}
\sum_r C(r) \propto \int_{-\infty}^{+\infty} dr \, e^{-|r|/\xi} = 2 \int_{0}^{+\infty} dr \, e^{-r/\xi} = 2\xi
\end{equation}

Questo mostra che la d suscettività è direttamente proporzionale alla lunghezza di correlazione. Dai calcoli espliciti si trova che la lunghezza di correlazione è $\xi = \frac{1}{2} e^{2\beta J}$, che, inserita nella relazione precedente, ci porta a:

\begin{equation}
\chi_m \propto \beta (2\xi) = \beta \left(2 \cdot \frac{1}{2} e^{2\beta J}\right) = \beta e^{2\beta J}
\end{equation}

Questo spiega perché $\chi_m$ diverge esponenzialmente a basse temperature: è un riflesso del fatto che la lunghezza di correlazione $\xi$ sta divergendo.


\subsection{Autovettori e il Loro Significato Fisico}

La matrice di trasferimento contiene ancora più informazioni. Oltre agli autovalori, anche i suoi autovettori hanno un preciso significato fisico. Poiché $T$ è simmetrica e reale, possiamo scomporla spettralmente:

\begin{equation}
T(S, \tilde{S}) = \lambda_1 v_1(S) v_1(\tilde{S}) + \lambda_2 v_2(S) v_2(\tilde{S})
\end{equation}

dove $v_1$ e $v_2$ sono gli autovettori ortonormali.


La probabilità marginale di trovare un singolo spin nello stato $S_1$, $p(S_1)$, si ottiene sommando su tutti gli altri spin. Nel formalismo della matrice di trasferimento, questo porta a:

\begin{equation}
p(S_1) = \frac{T^N(S_1, S_1)}{Z}
\end{equation}

Utilizzando la decomposizione spettrale e considerando il limite $N \to \infty$, dove $\lambda_1$ domina:

\begin{equation}
T^N(S_1, S_1) \approx \lambda_1^N v_1(S_1) v_1(S_1)
\end{equation}

Poiché $Z \approx \lambda_1^N$, otteniamo un risultato notevole:

\begin{equation}
p(S_1) = [v_1(S_1)]^2
\end{equation}

Il quadrato delle componenti dell'autovettore principale fornisce le probabilità marginali dello spin. 
Per il modello di Ising, la probabilità marginale $p(S)$ di un singolo spin, parametrizzata tramite la magnetizzazione media $m=<S>$, è data da:


\begin{equation}
p(S) =
\begin{cases}
\frac{1+m}{2} & \text{, se } S = +1 \\
\frac{1-m}{2} & \text{, se } S = -1
\end{cases}
\end{equation}

Gli autovettori, quindi, assumono la forma:

\begin{equation}
v_1 = \left( \sqrt{\frac{1+m}{2}}, \sqrt{\frac{1-m}{2}} \right), \quad v_2 = \left( \sqrt{\frac{1-m}{2}}, -\sqrt{\frac{1+m}{2}} \right)
\end{equation}

È facile dimostrare che:
\[v_1^2(s)=\dfrac{1+ms}{2}\]
\[\sum_{s=\{\pm1\}}v_{1,2}^2(s)=1\]
\begin{equation}\label{eqn:1}
    \sum_{s=\{\pm1\}}sv_1(s)v_1(s)=m
\end{equation}
\begin{equation}\label{eqn:2}
    \sum_{s=\{\pm1\}}sv_1(s)v_2(s)=\sqrt{1-m^2}
\end{equation}

\subsection{Funzione di Correlazione}

Il secondo autovalore, $\lambda_2$, che finora non abbiamo utilizzato, gioca un ruolo cruciale nel determinare il decadimento della funzione di correlazione. Per dimostrarlo, calcoliamo il valore di aspettazione $\langle S_i S_j \rangle$ utilizzando il formalismo della matrice di trasferimento. L'idea è di inserire gli operatori di spin $S_i$ e $S_j$ nella catena di matrici che costituisce la funzione di partizione.

\begin{equation}
\langle S_i S_j \rangle = \frac{\sum_{\underline{S}} S_i S_j e^{-\beta H}}{Z} = \frac{1}{Z} \text{Tr} \left( T \cdots T S_i T \cdots T S_j T \cdots T \right)
\end{equation}

Raggruppando i prodotti di matrici, l'espressione diventa:

\begin{equation}
\langle S_i S_j \rangle = \frac{1}{Z} \sum_{S_i, S_j} S_i T^{j-i}(S_i, S_j) S_j T^{N-(j-i)}(S_j, S_i)
\end{equation}

Per semplicità e sfruttando l'invarianza traslazionale, poniamo $i=0$ e $j=r$. L'espressione da calcolare è quindi:

\begin{equation}
\langle S_0 S_r \rangle = \frac{1}{Z} \sum_{S_0, S_r} S_0 T^r(S_0, S_r) S_r T^{N-r}(S_r, S_0)
\end{equation}

Ora sostituiamo la decomposizione spettrale di $T^k(S, \tilde{S}) = \sum_{\alpha=1,2} \lambda_\alpha^k v_\alpha(S) v_\alpha(\tilde{S})$ all'interno della somma. L'espressione diventa un prodotto di quattro termini, due per $T^r$ e due per $T^{N-r}$.

\begin{equation}
\begin{split}
\langle S_0 S_r \rangle = Z^{-1} \sum_{S_0, S_r} S_0 \left( \lambda_1^r v_1(S_0)v_1(S_r) + \lambda_2^r v_2(S_0)v_2(S_r) \right) \times \\
S_r \left( \lambda_1^{N-r} v_1(S_r)v_1(S_0) + \lambda_2^{N-r} v_2(S_r)v_2(S_0) \right)
\end{split}
\end{equation}

Nel limite termodinamico ($N \to \infty$), consideriamo che $Z \approx \lambda_1^N$. I termini dominanti nella somma sono quelli che contengono $\lambda_1^N$. Si hanno due contributi principali:
\begin{enumerate}
    \item Il prodotto dei termini con $\lambda_1$ da entrambe le parentesi.
    \item I prodotti "incrociati" che combinano $\lambda_1^r$ con $\lambda_2^{N-r}$ e $\lambda_2^r$ con $\lambda_1^{N-r}$.
\end{enumerate}

Analizziamo il primo contributo:
\begin{equation}
Z^{-1} \lambda_1^r \lambda_1^{N-r} \left( \sum_{S_0} S_0 v_1(S_0)^2 \right) \left( \sum_{S_r} S_r v_1(S_r)^2 \right)
\end{equation}
Poiché $Z \approx \lambda_1^N$, il fattore $\lambda_1^N$ si cancella. Ricordando che $v_1(S)^2 = p(S)$, le somme non sono altro che il valore medio dello spin, $\langle S \rangle = m$. Questo termine è quindi $m^2$, ovvero $\langle S_0 \rangle \langle S_r \rangle$.

Analizziamo ora uno dei termini incrociati (l'altro è identico):
\begin{equation}
Z^{-1} \lambda_2^r \lambda_1^{N-r} \left( \sum_{S_0} S_0 v_2(S_0) v_1(S_0) \right) \left( \sum_{S_r} S_r v_2(S_r) v_1(S_r) \right)
\end{equation}
Il fattore $Z^{-1} \lambda_1^{N-r} = (\lambda_1^N)^{-1} \lambda_1^{N-r} = \lambda_1^{-r}$ si combina con $\lambda_2^r$ per dare $(\frac{\lambda_2}{\lambda_1})^r$. Le due somme sono identiche e il loro quadrato è $(\sum_S S v_1(S) v_2(S))^2$. Svolgendo il calcolo si trova che questo termine vale $(1-m^2)$.

Mettendo insieme i pezzi, otteniamo che la funzione di correlazione connessa, 

$C^c(r)=\langle S_0 S_r \rangle - \langle S_0 \rangle \langle S_r \rangle$, è data dal solo termine incrociato:

\begin{equation}
C(r) = (1 - m^2) \left( \frac{\lambda_2}{\lambda_1} \right)^r
\end{equation}

Questo risultato è fondamentale: dimostra che la correlazione decade esponenzialmente con la distanza $r$. Il tasso di decadimento è controllato dal rapporto tra il secondo e il primo autovalore, definendo una \textbf{lunghezza di correlazione} $\xi$:

\begin{equation}
C(r) \propto e^{-r/\xi} \quad \text{con} \quad \xi = - \frac{1}{\log(|\lambda_2/\lambda_1|)}
\end{equation}

Un punto critico, dove le correlazioni diventano a lungo raggio ($\xi \to \infty$), corrisponde alla situazione in cui i due autovalori diventano degeneri, $\lambda_2 \to \lambda_1$.


